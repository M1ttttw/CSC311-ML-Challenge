{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS4EYE_xaX_d"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcydPKshZTVp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e52d94ea-e3ab-4454-92c1-be79ff7492bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "# This imports ***YOUR*** google drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHBwxIc4W0OL"
      },
      "outputs": [],
      "source": [
        "# 1) Download the clean_dataset.csv from quercus\n",
        "# 2) Import it to your google drive\n",
        "# 3) Move it to a folder called \"csc311\"\n",
        "# 4) Run the above code, give permission\n",
        "# 5) Run this code, it should give no errors if it can find the dataset\n",
        "# Then you can access the dataset in code by \"clean_dataset.csv\"\n",
        "![[ -f '/content/drive/MyDrive/csc311/clean_dataset.csv' ]] || echo \"Couldn't find clean_dataset.csv\"\n",
        "![[ -f '/content/drive/MyDrive/csc311/clean_dataset.csv' ]] && cp '/content/drive/MyDrive/csc311/clean_dataset.csv' 'clean_dataset.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sVtLigZbVLV"
      },
      "outputs": [],
      "source": [
        "# This is the starter code from challenge_basic.py on quercus\n",
        "\"\"\"\n",
        "This Python file provides some useful code for reading the training file\n",
        "\"clean_dataset.csv\". You may adapt this code as you see fit. However,\n",
        "keep in mind that the code provided does only basic feature transformations\n",
        "to build a rudimentary kNN model in sklearn. Not all features are considered\n",
        "in this code, and you should consider those features! Use this code\n",
        "where appropriate, but don't stop here!\n",
        "\"\"\"\n",
        "from pprint import pprint\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "file_name = \"clean_dataset.csv\"\n",
        "random_state = 42\n",
        "\n",
        "def to_numeric(s):\n",
        "    \"\"\"Converts string `s` to a float.\n",
        "\n",
        "    Invalid strings and NaN values will be converted to float('nan').\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(s, str):\n",
        "        s = s.replace(\",\", '')\n",
        "        s = pd.to_numeric(s, errors=\"coerce\")\n",
        "    return float(s)\n",
        "\n",
        "def get_number_list(s):\n",
        "    \"\"\"Get a list of integers contained in string `s`\n",
        "    \"\"\"\n",
        "    return [int(n) for n in re.findall(\"(\\d+)\", str(s))]\n",
        "\n",
        "def get_number_list_clean(s):\n",
        "    \"\"\"Return a clean list of numbers contained in `s`.\n",
        "\n",
        "    Additional cleaning includes removing numbers that are not of interest\n",
        "    and standardizing return list size.\n",
        "    \"\"\"\n",
        "\n",
        "    n_list = get_number_list(s)\n",
        "    n_list += [-1]*(6-len(n_list))\n",
        "    return n_list\n",
        "\n",
        "def get_number(s):\n",
        "    \"\"\"Get the first number contained in string `s`.\n",
        "\n",
        "    If `s` does not contain any numbers, return -1.\n",
        "    \"\"\"\n",
        "    n_list = get_number_list(s)\n",
        "    return n_list[0] if len(n_list) >= 1 else -1\n",
        "\n",
        "def find_area_at_rank(l, i):\n",
        "    \"\"\"Return the area at a certain rank in list `l`.\n",
        "\n",
        "    Areas are indexed starting at 1 as ordered in the survey.\n",
        "\n",
        "    If area is not present in `l`, return -1.\n",
        "    \"\"\"\n",
        "    return l.index(i) + 1 if i in l else -1\n",
        "\n",
        "def cat_in_s(s, cat):\n",
        "    \"\"\"Return if a category is present in string `s` as an binary integer.\n",
        "    \"\"\"\n",
        "    return int(cat in s) if not pd.isna(s) else 0\n",
        "\n",
        "\n",
        "def make_bow(data, vocab):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    X = np.zeros([len(data), len(vocab)])\n",
        "    mapping = {word: i for i, word in enumerate(vocab)}\n",
        "    for i, review in enumerate(data):\n",
        "        if review is np.nan:\n",
        "          continue\n",
        "        for word in review.split():\n",
        "            if word in mapping.keys():\n",
        "                X[i, mapping[word]] = 1\n",
        "    return X\n",
        "\n",
        "\n",
        "\n",
        "def readData(filename, vocab):\n",
        "    '''\n",
        "    Read <filename> into dataframes x (normalized) and y\n",
        "    For training/computing weights, set vocab=None.\n",
        "    For predicting, pass in vocab.\n",
        "    '''\n",
        "    df = pd.read_csv(filename)\n",
        "    # scatter plots\n",
        "    questions = ['Q1', 'Q2', 'Q3', 'Q4']\n",
        "    #for f1 in questions:\n",
        "    for _ in []:\n",
        "        f2 = 'Label'\n",
        "        plt.figure()\n",
        "        plt.scatter(df[f1], df[f2], alpha=0.1)\n",
        "        plt.xlabel(f1)\n",
        "        plt.ylabel(f2)\n",
        "        plt.title(f\"{f1} vs {f2}\")\n",
        "        plt.axis('scaled')\n",
        "    #explore the data by blox plots\n",
        "    #des = df.describe()\n",
        "    #print(des)\n",
        "    #for fet in [\"Q1\", \"Q2\", \"Q3\",\n",
        "            #\"Q4\", \"Q8\"]:\n",
        "      #df.boxplot(column=fet, by='Label')\n",
        "\n",
        "    # Clean numerics\n",
        "\n",
        "    df[\"Q7\"] = df[\"Q7\"].apply(to_numeric).fillna(0) # fill empty with 0, numeric temperature\n",
        "    df[\"Q8\"] = df[\"Q8\"].apply(to_numeric).fillna(0)\n",
        "    df[\"Q9\"] = df[\"Q9\"].apply(to_numeric).fillna(0)\n",
        "    questions = ['Q7', 'Q8', 'Q9']\n",
        "    #for f1 in questions:\n",
        "    for _ in []:\n",
        "        f2 = 'Label'\n",
        "        plt.figure()\n",
        "        plt.scatter(df[f1], df[f2], alpha=0.1)\n",
        "        plt.xlabel(f1)\n",
        "        plt.ylabel(f2)\n",
        "        plt.title(f\"{f1} vs {f2}\")\n",
        "\n",
        "    # Clean for number categories\n",
        "\n",
        "    df[\"Q1\"] = df[\"Q1\"].apply(get_number)\n",
        "    df[\"Q2\"] = df[\"Q2\"].apply(get_number)\n",
        "    df[\"Q3\"] = df[\"Q3\"].apply(get_number)\n",
        "    df[\"Q4\"] = df[\"Q4\"].apply(get_number)\n",
        "\n",
        "    # Create area rank categories\n",
        "\n",
        "    df[\"Q6\"] = df[\"Q6\"].apply(get_number_list_clean) # extract all numbers in the Q6 answer string (skyscrapers=>6)\n",
        "    #print(df['Q6'])\n",
        "\n",
        "    temp_names = []\n",
        "    for i in range(1,7):\n",
        "        col_name = f\"rank_{i}\"\n",
        "        temp_names.append(col_name)\n",
        "        df[col_name] = df[\"Q6\"].apply(lambda l: find_area_at_rank(l, i))\n",
        "\n",
        "    del df[\"Q6\"]\n",
        "\n",
        "    # Create category indicators\n",
        "\n",
        "    new_names = []\n",
        "    for col in [\"Q1\", \"Q2\", \"Q3\", \"Q4\"] + temp_names:\n",
        "        indicators = pd.get_dummies(df[col], prefix=col)\n",
        "        new_names.extend(indicators.columns)\n",
        "        df = pd.concat([df, indicators], axis=1)\n",
        "        del df[col]\n",
        "        for i in range(-1,7):\n",
        "          newname=col+'_'+str(i)\n",
        "          if not newname in df.keys():\n",
        "            df[newname]=0 # initialize columns\n",
        "            new_names.append(newname)\n",
        "\n",
        "    # Create multi-category indicators\n",
        "\n",
        "    for cat in [\"Partner\", \"Friends\", \"Siblings\", \"Co-worker\"]:\n",
        "      cat_name = f\"Q5{cat}\"\n",
        "      new_names.append(cat_name)\n",
        "      df[cat_name] = df[\"Q5\"].apply(lambda s: cat_in_s(s, cat))\n",
        "\n",
        "    del df[\"Q5\"]\n",
        "\n",
        "    # bags of words for Q10\n",
        "    df['Q10'] = df['Q10'].str.lower()\n",
        "    if vocab==None:\n",
        "      df['Q10'].fillna('', inplace=True)# replace all missing text in Q10 by ''\n",
        "      vocab = []\n",
        "      for i, line in enumerate(df['Q10']):\n",
        "        line = re.sub(r'\\W+', ' ', line)\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "          if word not in vocab:\n",
        "            vocab.append(word)\n",
        "\n",
        "      with open('/content/drive/My Drive/csc311/vocab.txt', 'w') as file:\n",
        "          for word in vocab:\n",
        "              file.write(word + '\\n')\n",
        "\n",
        "    Q10_matrix = make_bow(df['Q10'], vocab)\n",
        "    dfq10 = pd.DataFrame(Q10_matrix, columns=vocab)\n",
        "\n",
        "    df = df.sample(frac=1, random_state=random_state) # permute the rows randomly\n",
        "\n",
        "    if \"Label\" in df.keys():\n",
        "      y = pd.get_dummies(df[\"Label\"].values)\n",
        "    else:\n",
        "      y = [] # shouldn't be used by predict_all anyways\n",
        "    df = df[new_names + [\"Q7\", \"Q8\", \"Q9\"]]\n",
        "\n",
        "    df = pd.concat([df, dfq10], axis=1) #integrate df and df3\n",
        "\n",
        "    x = df\n",
        "    x = np.array(x,dtype=float)\n",
        "\n",
        "    # Normalize\n",
        "\n",
        "    def normalize(col):\n",
        "      idx=list(df.keys()).index(col)\n",
        "\n",
        "      vals=x[:, idx]\n",
        "      mean = vals.mean()\n",
        "      std = vals.std()\n",
        "      if std==0:\n",
        "        std=1e-10\n",
        "      x[:, idx] = (vals - mean) / std\n",
        "    normalize(\"Q7\")\n",
        "    normalize(\"Q8\")\n",
        "    normalize(\"Q9\")\n",
        "\n",
        "    return x,y\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  x,y=readData(file_name,None)\n",
        "\n",
        "  # test-train split\n",
        "  n_train = 1467\n",
        "\n",
        "  x_train = x[:n_train]\n",
        "  y_train = y[:n_train]\n",
        "\n",
        "  x_test = x[n_train:]\n",
        "  y_test = y[n_train:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "TUv_-tPsAdPt",
        "outputId": "9d31a520-ba58-4568-bff6-8145d2bf77c2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-30b47604c6c0>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbest_test_acc\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mbest_test_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \"\"\"\n\u001b[1;32m    425\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;34m\"\"\"Validate the training data on predict (probabilities).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             if issparse(X) and (\n\u001b[1;32m    394\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# Use NumPy API to support order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(self, x, dtype, device, copy)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munique_inverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "d = 100\n",
        "s = 100\n",
        "\n",
        "crit = [\"gini\", \"entropy\", \"log_loss\"]\n",
        "best_params = [0, -1, -1] # [crit, mdepth, mi_split]\n",
        "best_test_acc = 0\n",
        "\n",
        "def_mdepth = None\n",
        "def_mi_split = 2\n",
        "\n",
        "for i in range(3):\n",
        "    for j in range(1, d):\n",
        "        clf = DecisionTreeClassifier(criterion = crit[i], max_depth=j)\n",
        "        clf.fit(x_train, y_train)\n",
        "        train_acc = clf.score(x_train, y_train)\n",
        "        test_acc = clf.score(x_test, y_test)\n",
        "        if best_test_acc < test_acc:\n",
        "            best_test_acc = test_acc\n",
        "            best_params[0] = crit[i]\n",
        "            best_params[1] = j\n",
        "            best_params[2] = def_mi_split\n",
        "    if best_params[1] == -1:\n",
        "        best_params[1] = def_mdepth\n",
        "\n",
        "    for j in range(2, s):\n",
        "        clf = DecisionTreeClassifier(criterion = crit[i], max_depth=best_params[1], min_samples_split = j)\n",
        "        clf.fit(x_train, y_train)\n",
        "        train_acc = clf.score(x_train, y_train)\n",
        "        test_acc = clf.score(x_test, y_test)\n",
        "        if best_test_acc < test_acc:\n",
        "            best_test_acc = test_acc\n",
        "            best_params[0] = crit[i]\n",
        "            best_params[2] = j\n",
        "    if best_params[1] == -1:\n",
        "        best_params[1] = def_mdepth\n",
        "\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "print(f\"{type(clf).__name__} test acc: {best_test_acc}\")\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion = best_params[0], max_depth=best_params[1], min_samples_split = best_params[2])\n",
        "clf.fit(x_train, y_train)\n",
        "train_acc = clf.score(x_train, y_train)\n",
        "test_acc = clf.score(x_test, y_test)\n",
        "print(f\"{type(clf).__name__} test acc: {test_acc}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "7_a2qTiOwZZZ",
        "outputId": "2a0c0ac6-c6f3-47ef-feb2-5acc56ddab0d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d17e410decbe>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbest_test_acc\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mbest_test_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \"\"\"\n\u001b[0;32m--> 820\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    871\u001b[0m         ]\n\u001b[1;32m    872\u001b[0m         \u001b[0mlock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m         Parallel(n_jobs=n_jobs, verbose=self.verbose, require=\"sharedmem\")(\n\u001b[0m\u001b[1;32m    874\u001b[0m             \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_accumulate_prediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_accumulate_prediction\u001b[0;34m(predict, X, out, lock)\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mcomplains\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mit\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0mpickle\u001b[0m \u001b[0mit\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mplaced\u001b[0m \u001b[0mthere\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \"\"\"\n\u001b[0;32m--> 650\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "e = 100\n",
        "d = 100\n",
        "s = 100\n",
        "\n",
        "def_n_est = 42\n",
        "def_mdepth = 13\n",
        "def_mi_split = 2\n",
        "\n",
        "crit = [\"gini\", \"entropy\", \"log_loss\"]\n",
        "max_f = [\"sqrt\", \"log2\"]\n",
        "best_params = [0, 0, -1, -1, -1] # [crit, max_f, n_est, m_depth, mi_split]\n",
        "best_test_acc = 0\n",
        "\n",
        "for i in range(3):\n",
        "    for j in range(2):\n",
        "        # Test for best n estimators first\n",
        "        for k in range(89, 90):\n",
        "            # if best_params[3] != 0 and best_params[4] != 0:\n",
        "            #     clf = RandomForestClassifier(n_estimators = k, criterion = crit[i], max_depth = best_params[3], min_samples_split = best_params[4], max_features = max_f[j])\n",
        "            # else:\n",
        "            #     clf = RandomForestClassifier(n_estimators = k, criterion = crit[i], max_features = max_f[j])\n",
        "            clf = RandomForestClassifier(n_estimators = k, criterion = crit[i], max_features = max_f[j])\n",
        "            clf.fit(x_train, y_train)\n",
        "            train_acc = clf.score(x_train, y_train)\n",
        "            test_acc = clf.score(x_test, y_test)\n",
        "            if best_test_acc < test_acc:\n",
        "                best_test_acc = test_acc\n",
        "                best_params[0] = crit[i]\n",
        "                best_params[1] = max_f[j]\n",
        "                best_params[2] = k\n",
        "                best_params[3] = None\n",
        "                best_params[4] = 2\n",
        "        if best_params[2] == -1:\n",
        "            best_params[2] = def_n_est\n",
        "\n",
        "        # Test for best max depth\n",
        "        for k in range(1, d + 1):\n",
        "            # if best_params[2] != 0 and best_params[4] != 0:\n",
        "            #     clf = RandomForestClassifier(n_estimators = best_params[2], criterion = crit[i], max_depth = k, min_samples_split = best_params[4], max_features = max_f[j])\n",
        "            # else:\n",
        "            #     clf = RandomForestClassifier(n_estimators = best_params[2], criterion = crit[i], max_depth = k, max_features = max_f[j])\n",
        "            clf = RandomForestClassifier(n_estimators = best_params[2], criterion = crit[i], max_depth = k, max_features = max_f[j])\n",
        "            clf.fit(x_train, y_train)\n",
        "            train_acc = clf.score(x_train, y_train)\n",
        "            test_acc = clf.score(x_test, y_test)\n",
        "            if best_test_acc < test_acc:\n",
        "                best_test_acc = test_acc\n",
        "                best_params[0] = crit[i]\n",
        "                best_params[1] = max_f[j]\n",
        "                best_params[3] = k\n",
        "                best_params[4] = 2\n",
        "        if best_params[3] == -1:\n",
        "            best_params[3] = def_mdepth\n",
        "\n",
        "        # Finally, test for best min_sample_split\n",
        "        for k in range(2, s + 1):\n",
        "            clf = RandomForestClassifier(n_estimators = best_params[2], criterion = crit[i], max_depth = best_params[3], min_samples_split = k, max_features = max_f[j])\n",
        "            clf.fit(x_train, y_train)\n",
        "            train_acc = clf.score(x_train, y_train)\n",
        "            test_acc = clf.score(x_test, y_test)\n",
        "            if best_test_acc < test_acc:\n",
        "                best_test_acc = test_acc\n",
        "                best_params[0] = crit[i]\n",
        "                best_params[1] = max_f[j]\n",
        "                best_params[4] = k\n",
        "        if best_params[4] == -1:\n",
        "            best_params[4] = def_mi_split\n",
        "\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "print(f\"{type(clf).__name__} test acc: {best_test_acc}\")\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators = best_params[2], criterion = best_params[0], max_depth = best_params[3], min_samples_split = best_params[4], max_features = best_params[1])\n",
        "clf.fit(x_train, y_train)\n",
        "train_acc = clf.score(x_train, y_train)\n",
        "test_acc = clf.score(x_test, y_test)\n",
        "print(f\"{type(clf).__name__} test acc: {test_acc}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dICr6kN1V6WT"
      },
      "outputs": [],
      "source": [
        "def dataframe_to_targets(y):\n",
        "  '''\n",
        "  From Pandas \"DataFrame\" object, return the set of labels\n",
        "  A DataFrame is like a spreadsheet, a series of rows and columns. The labels are just the first row\n",
        "  '''\n",
        "  labels=y.keys()\n",
        "  targets=['']*len(y.values)\n",
        "\n",
        "  for r,row in enumerate(y.values):\n",
        "    for i,x in enumerate(row):\n",
        "      if x!=0:\n",
        "        targets[r]=labels[i]\n",
        "        break\n",
        "  return targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLumpnbd_LDq",
        "outputId": "d0ff281f-37e1-40f8-8ab6-e6ec419243de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression train acc: 0.9652351738241309\n",
            "LogisticRegression test acc: 1.0\n",
            "f1 score 1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint\n",
        "clf = LogisticRegression(max_iter=9000,fit_intercept=False, multi_class = 'multinomial', solver = 'newton-cg')\n",
        "\n",
        "#y_train2 = (y_train.iloc[:, 1:] == 1).idxmax(1)\n",
        "#y_test2 = (y_test.iloc[:, 1:] == 1).idxmax(1)\n",
        "\n",
        "y_train2 = dataframe_to_targets(y_train)\n",
        "y_test2 = dataframe_to_targets(y_test)\n",
        "\n",
        "clf.fit(x_train,y_train2)\n",
        "train_acc = clf.score(x_train, y_train2)\n",
        "test_acc = clf.score(x_test, y_test2)\n",
        "print(f\"{type(clf).__name__} train acc: {train_acc}\")\n",
        "print(f\"{type(clf).__name__} test acc: {test_acc}\")\n",
        "\n",
        "weights=np.array(clf.coef_, dtype=float)\n",
        "with open('/content/drive/My Drive/csc311/weights.txt', 'w') as file:\n",
        "    for row in weights:\n",
        "        line = ' '.join(map(str, row))  # Convert each weight to string and join by spaces\n",
        "        file.write(line + '\\n')\n",
        "predicted_labels = clf.predict(x_test)\n",
        "f1 = f1_score(y_test2, predicted_labels, average='weighted')\n",
        "print('f1 score', f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8PL-G1UIDaV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "932c34eb-bde2-440c-9420-99cf72f04b96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters {'batch_size': 80, 'max_iter': 20, 'learning_rate_init': 0.001} Best score: 0.891156462585034\n",
            "train acc 0.9531516183986372\n",
            "test acc 0.8843537414965986\n",
            "f1 socore 0.9188097690519028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "# clf = MLPClassifier(random_state=0, max_iter=30, batch_size=20,learning_rate_init=0.001).fit(x_train, y_train)\n",
        "# acc_train = clf.score(x_train, y_train)\n",
        "# acc_test = clf.score(x_test, y_test)\n",
        "# print(\"train acc\", acc_train)\n",
        "# print(\"test acc\", acc_test)\n",
        "def build_all_models_neural(batch_size,\n",
        "                     max_iter,\n",
        "                     learning_rate_init,\n",
        "                     random_state=[None],\n",
        "                     X_train=x_train,\n",
        "                     t_train=y_train,\n",
        "                     X_valid=x_test,\n",
        "                     t_valid=y_test):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        `max_depths` - A list of values representing the max_depth values to be\n",
        "                       try as hyperparameter values\n",
        "        `min_samples_split` - An list of values representing the min_samples_split\n",
        "                       values to try as hyperpareameter values\n",
        "        `criterion` -  A string; either \"entropy\" or \"gini\"\n",
        "\n",
        "    Returns a dictionary, `out`, whose keys are the the hyperparameter choices, and whose values are\n",
        "    the training and validation accuracies (via the `score()` method).\n",
        "    In other words, out[(max_depth, min_samples_split)]['val'] = validation score and\n",
        "                    out[(max_depth, min_samples_split)]['train'] = training score\n",
        "    For that combination of (max_depth, min_samples_split) hyperparameters.\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "\n",
        "    for b in batch_size:\n",
        "        for i in max_iter:\n",
        "          for a in learning_rate_init:\n",
        "            for j in random_state:\n",
        "              out[(b, i , a, j)] = {}\n",
        "              # Create a DecisionTreeClassifier based on the given hyperparameters and fit it to the data\n",
        "              clf = MLPClassifier(random_state=j, max_iter=i, learning_rate_init=a, batch_size=b).fit(X_train, t_train)# TODO\n",
        "              # TODO: store the validation and training scores in the `out` dictionary\n",
        "              out[(b, i , a, j)]['test'] = clf.score(X_valid, t_valid)\n",
        "              out[(b, i , a, j)]['train'] = clf.score(X_train, t_train)\n",
        "    return out\n",
        "batch_size = [80]\n",
        "max_iter = [20]\n",
        "learning_rate_init = [0.001]\n",
        "best_score1 = 0\n",
        "res = build_all_models_neural(batch_size, max_iter, learning_rate_init,X_train=x_train,\n",
        "                     t_train=y_train,\n",
        "                     X_valid=x_test,\n",
        "                     t_valid=y_test)\n",
        "for b, i, a, j in res:\n",
        "      test_score = res[(b, i , a, j)]['test']\n",
        "      if test_score > best_score1:\n",
        "        best_score1 = test_score\n",
        "        best_para1 = {'batch_size': b,\n",
        "                     'max_iter': i,\n",
        "                     'learning_rate_init': a,\n",
        "                     }\n",
        "print(\"Best parameters\", best_para1, \"Best score:\", best_score1)\n",
        "clf = MLPClassifier(random_state=0, max_iter=20, batch_size=80,learning_rate_init=0.001).fit(x_train, y_train)\n",
        "acc_train = clf.score(x_train, y_train)\n",
        "acc_test = clf.score(x_test, y_test)\n",
        "print(\"train acc\", acc_train)\n",
        "print(\"test acc\", acc_test)\n",
        "predicted_labels = clf.predict(x_test)\n",
        "f1 = f1_score(y_test, predicted_labels, average='weighted')\n",
        "print('f1 socore', f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q__O_0dSHqj4",
        "outputId": "4646e4cd-f17a-4d65-a81e-0d926f9ca03b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of mislabeled points out of a total 294 points :  103  (acc  0.6496598639455782 )\n",
            "{'priors': None, 'var_smoothing': 1e-09}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "gnb = GaussianNB()\n",
        "y_pred = gnb.fit(x_train, dataframe_to_targets(y_train)).predict(x_test)\n",
        "mislabelled = (dataframe_to_targets( y_test ) != y_pred).sum()\n",
        "print(\"Number of mislabeled points out of a total\", len(y_test),\"points : \",  mislabelled, ' (acc ', 1-(mislabelled/len(y_test)), ')')\n",
        "print(gnb.get_params(deep=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wxN8_wFUiQA"
      },
      "outputs": [],
      "source": [
        "def my_logistic_regression(x_data,weights):\n",
        "    '''\n",
        "    myu: see https://en.wikipedia.org/wiki/Logistic_regression\n",
        "    s:   see https://en.wikipedia.org/wiki/Logistic_regression\n",
        "    '''\n",
        "    cities = np.array(['Dubai', 'New York City', 'Paris', 'Rio de Janeiro'])\n",
        "    x_data=np.array(x_data,dtype=float)\n",
        "    z = np.matmul(weights, x_data.T, dtype=float)\n",
        "\n",
        "    m_vec = np.max(z, axis = 0) # Should be a (N, 1) Vector\n",
        "    e_z = np.exp(z - m_vec)\n",
        "\n",
        "    sum_e_z_vec = np.sum(e_z, axis = 0)\n",
        "\n",
        "    y_temp = e_z / sum_e_z_vec # e_z is (N, K), sum_e_z is (n, 1)\n",
        "\n",
        "    y = [cities[j] for j in np.argmax(y_temp, axis = 0)]\n",
        "\n",
        "    return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5UN5z7Mcppq",
        "outputId": "d4ac933c-74b9-45d2-9586-e8db94cc25ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test 0.8877551020408163\n",
            "entire file 0.8596730245231607\n"
          ]
        }
      ],
      "source": [
        "# This is the starter code from example_pred.py on quercus\n",
        "\"\"\"\n",
        "This Python file is example of how your `pred.py` script should\n",
        "look. Your file should contain a function `predict_all` that takes\n",
        "in the name of a CSV file, and returns a list of predictions.\n",
        "\n",
        "Your `pred.py` script can use different methods to process the input\n",
        "data, but the format of the input it takes and the output your script produces should be the same.\n",
        "\n",
        "Here's an example of how your script may be used in our test file:\n",
        "\n",
        "    from example_pred import predict_all\n",
        "    predict_all(\"example_test_set.csv\")\n",
        "\"\"\"\n",
        "\n",
        "# basic python imports are permitted\n",
        "import sys\n",
        "import csv\n",
        "import random\n",
        "\n",
        "# numpy and pandas are also permitted\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "WEIGHTS_FILE='/content/drive/My Drive/csc311/weights.txt'\n",
        "VOCAB_FILE='/content/drive/My Drive/csc311/vocab.txt'\n",
        "\n",
        "def predict(x,weights):\n",
        "    \"\"\"\n",
        "    Helper function to make prediction for a given input x.\n",
        "    This code is here for demonstration purposes only.\n",
        "    \"\"\"\n",
        "    # randomly choose between the four choices: 'Dubai', 'Rio de Janeiro', 'New York City' and 'Paris'.\n",
        "    # NOTE: make sure to be *very* careful of the spelling/capitalization of the cities!!\n",
        "\n",
        "    # return the prediction\n",
        "    return my_logistic_regression(x,weights)\n",
        "\n",
        "def random_predict(x):\n",
        "    \"\"\"\n",
        "    Helper function to make prediction for a given input x.\n",
        "    This code is here for demonstration purposes only.\n",
        "    \"\"\"\n",
        "    # randomly choose between the four choices: 'Dubai', 'Rio de Janeiro', 'New York City' and 'Paris'.\n",
        "    # NOTE: make sure to be *very* careful of the spelling/capitalization of the cities!!\n",
        "    y = random.choice(['Dubai', 'Rio de Janeiro', 'New York City' ,'Paris'])\n",
        "\n",
        "    # return the prediction\n",
        "    return y\n",
        "\n",
        "def load_weights_vocab(weights_filename, vocab_filename):\n",
        "  '''\n",
        "  Import weights and vocab from external files, to keep pred.py clean\n",
        "  '''\n",
        "  weights = []\n",
        "  with open(weights_filename, 'r') as file:\n",
        "      for line in file:\n",
        "          row = list(map(float, line.split()))  # Split by space and convert back to floats\n",
        "          weights.append(row)\n",
        "\n",
        "  weights = np.array(weights, dtype=float)\n",
        "\n",
        "  vocab = []\n",
        "  with open(vocab_filename, 'r') as file:\n",
        "    for line in file:\n",
        "      vocab.append(line)\n",
        "\n",
        "\n",
        "  return weights, vocab\n",
        "\n",
        "def predict_all(filename):\n",
        "    \"\"\"\n",
        "    Make predictions for the data in filename\n",
        "    \"\"\"\n",
        "    weights, vocab = load_weights_vocab(WEIGHTS_FILE, VOCAB_FILE)\n",
        "    x,y=readData(filename,vocab)\n",
        "\n",
        "    return predict(x,weights)\n",
        "\n",
        "def accuracy(t,y):\n",
        "  '''\n",
        "  predictions t, real targets y\n",
        "  '''\n",
        "  return 1-sum(np.array(t) != np.array(y))/len(y)\n",
        "\n",
        "# test code\n",
        "if __name__=='__main__':\n",
        "  weights, vocab = load_weights_vocab(WEIGHTS_FILE, VOCAB_FILE)\n",
        "  predictions = predict(x_test,weights)\n",
        "  preds = predict_all(file_name)\n",
        "  print('test',end=' ')\n",
        "  pprint(accuracy(predictions,y_test2))\n",
        "  y2 = dataframe_to_targets(y)\n",
        "  print('entire file',end=' ')\n",
        "  pprint(accuracy(preds,y2))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}